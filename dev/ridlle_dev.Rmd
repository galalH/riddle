---
title: "ridlle functions"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r development, include=FALSE}
library(testthat)
#library(riddle)
```

```{r development-load}
# Load already included functions if relevant
pkgload::load_all(export_all = FALSE)
```

```{r development-data}
# library(tidyverse)
# keywords <-
#   jsonlite::fromJSON("https://raw.githubusercontent.com/okfn/ckanext-unhcr/master/ckanext/unhcr/schemas/dataset.json") %>%
#   purrr::pluck("dataset_fields") %>%
#   dplyr::filter(field_name == "keywords") %>%
#   purrr::pluck("choices", 1) %>%
#   tibble::deframe()
# 
# usethis::use_data(keywords, overwrite = TRUE)
# #sinew::makeOxygen(keywords, add_fields = "source")

## here is a list of all keywords used for dataset documentation
knitr::kable(riddle::keywords)
```




# ridl
    
```{r function-ridl}
#' apihelper
#' 
#' Helper function to make [API calls](https://docs.ckan.org/en/2.9/api/index.html#action-api-reference).
#'  Calls includes the 10 following actions:
#' 
#' On dataset
#'  * "package_create"
#'  * "package_update"
#'  * "package_patch"
#'  * "package_delete"
#'  * "package_search"
#'  
#' On resource  
#'  * "resource_create"
#'  * "resource_update"
#'  * "resource_patch"
#'  * "resource_delete"
#'  * "resource_search"
#'
#' The package works with both the production and UAT instances of RIDL. 
#' To use the UAT version, run `Sys.setenv(USE_UAT=1)` before calling any functions
#'  from the package.
#'  To go back to the production instance, call `Sys.unsetenv("USE_UAT")`.
#'
#'
#' @param action Operation to execute. See [CKAN's API documentation](https://docs.ckan.org/en/2.9/api/) for details.
#' @param .encoding HTTP POST encoding to use - one of `json`, `form`, or `multipart`.
#' @param ... whatever is needed
#' @param verbose TRUE FALSE to display info on the console about the API call
#' @importFrom httr POST
#'
#'
#' @return `httr::response` object with the result of the call.
#' @export

ridl <- function(action,
                 ...,
                 .encoding = "json",
                 verbose = FALSE) {
     # if(Sys.setenv(USE_UAT=1))  {
      if(verbose == TRUE) {print( c("Running ridl action", action))}
      

      # It's ok to check the environment using Sys.setenv()?
      # Should Sys.getenv() will be better?
      if(Sys.getenv("USE_UAT") == 1)  {
        token <- Sys.getenv("RIDL_UAT_API_TOKEN")
        if(verbose == TRUE) {print(" - env: UAT")}
        #print(" - env: UAT")
        #nchar(token) - show the end of the token as the beginning is more likely to be the same... 
        if(verbose == TRUE) {print( c(" - key", substr(token, (nchar(token)-5) , nchar(token))  ) )}
        #print( c(" - key", substr(token, (nchar(token)-5) , nchar(token))  ) )
      
        r <- httr::POST("https://ridl-uat.unhcr.org/",
                 path = glue::glue("/api/action/{action}"),
               # httr::add_headers("Authorization" = Sys.getenv("RIDL_API_KEY_UAT")),
               #  httr::add_headers("Authorization" = Sys.getenv("RIDL_UAT_API_TOKEN")),
                 httr::add_headers("X-CKAN-API-Key" = token),
                 body = rlang::list2(...),
                 encode = .encoding) %>%
            httr::content(simplifyVector = TRUE)
      
    } else    {
      
      token <- Sys.getenv("RIDL_API_TOKEN")
        if(verbose == TRUE) {print(" - env: PRODUCTION")}
        # print(" - env: PRODUCTION")
      
      #nchar(token) - show the end of the token as the beginning is more likely to be the same... 
        if(verbose == TRUE) { print( c(" - key", substr(token, (nchar(token)-5) , nchar(token))  ) )}
       #  print( c(" - key", substr(token, (nchar(token)-5) , nchar(token))  ) )
    
      r <- httr::POST("https://ridl.unhcr.org/",
                 path = glue::glue("/api/action/{action}"),
                # httr::add_headers("Authorization" = Sys.getenv("RIDL_API_KEY")),
                 # httr::add_headers("Authorization" = Sys.getenv("RIDL_API_TOKEN")),
                 httr::add_headers("X-CKAN-API-Key" = token),
                 body = rlang::list2(...),
                 encode = .encoding) %>%
            httr::content(simplifyVector = TRUE) 
      
       }
  
  ## Display results in console
    if(verbose == TRUE) {message(r$result)}
  
  ## Error catching
  if (!r$success)
    stop(r$error %>%
           purrr::imap(~glue::glue("{.y}: {.x}")) %>%
           unlist() %>%
           stringr::str_c(collapse = "\n"))
  
  ## Return all the details of the API interaction... 
  return(r)
}


```
  
```{r example-ridl}
# ridl(action ="package_search", as.list("cbi"))
```
  
```{r tests-ridl}
test_that("ridl works", {   
  expect_true(inherits(ridl, "function")) 
  ## GPT unit test
  #expect_equal(ridl(action = "test"), "test")
 # expect_error(ridl(action = "invalid"), "Invalid action: invalid")
  })
```

# container_show
    
```{r function-container_show}
#' container_show  
#' 
#' Get an overview of accessible infos about all containers in RIDL
#'  Use https://docs.ckan.org/en/2.9/api/index.html#ckan.logic.action.get.organization_show  
#' @param id id or name of the container
#' @return a dataframe with container metadata
#' 
#' @export
container_show <- function(id) {
      ridl(action ="organization_show",
           include_datasets = TRUE,
           id = id)-> r
    r$result %>% 
    dataset_tibblify() -> res
}
```
  
```{r example-container_show}
# americasdataset <- container_show( id = "americas-regional-dataset")
```
  
```{r tests-container_show}
test_that("container_show works", {
  expect_true(inherits(container_show, "function")) 
})
```


# container_list
    
```{r function-container_list}
#' container_list
#' 
#' Provide a list of all child containers for a specific container
#' 
#' This function is used to generate a regional dashbaord..
#' Be carefull - it's an expansive functions at it needs to parse the entire 
#' content of the server.... 
#' 
#' uses https://docs.ckan.org/en/2.9/api/index.html#ckan.logic.action.get.organization_list
#' @param parent name of the container
#' @return catalog of containers
#' 
#' @importFrom tibble as_tibble
#' @importFrom tidyr unnest_wider
#' @importFrom purrr map list_rbind
#' 
#' @export
container_list <- function(parent) {
    
    ## First we list all the containers in the server
    container_get <- function() {
      ridl(action ="organization_list",
           type = "data-container" ) -> r
      r$result  |> length() -> res
    
      return(res)  }
    cont <- container_get()
    
    ## Now we build the full catalog 
    catalog <-
      purrr::map(seq(0, cont, by = 25),
          \(x) ridl(action ="organization_list",
                    type = "data-container",
                    offset = x,
                    all_fields = TRUE,
                    include_groups = TRUE)  %>% .[["result"]]) |>
      purrr::list_rbind() |>
      tidyr::unnest_wider(groups, names_sep = "_") |>
      tibble::as_tibble()
    
    return(catalog)
}
 
```
  
```{r example-container_list}
# catalog <- container_list()
# groups_name <- catalog |>
#                   dplyr::select(groups_name) |>
#                   dplyr::distinct()

```
  
```{r tests-container_list}
test_that("container_list works", {
  expect_true(inherits(container_list, "function")) 
})
```
  
  
# find_child_containers
    
```{r function-find_child_containers}
#' find_child_containers
#' 
#' Provide a list of all child containers - including nested one - for a specific container
#' 
#' Be carefull - it's an expansive functions at it needs to parse the entire 
#' content of the server.... 
#' 
#' @param parent name of the parent container
#' @param catalog daaframe object with a catalog of container produced by container_list()
#' 
#' @return vector with all child container
#' 
#' @importFrom dplyr filter
#' @importFrom rlang is_empty
#' @importFrom purrr map
#' 
#' @export
find_child_containers <- function(parent, catalog ) {
   # catalog <- container_list()
    
    find_child_containers2 <- function(parent) {
    ## Now we filter the catalog only for the children of the desired container
      children <- dplyr::filter(catalog, groups_name == parent)$name
      if (rlang::is_empty(children))
        return(NULL)
      c(children, unlist(purrr::map(children, find_child_containers2))) }
    
   result <-  find_child_containers2(parent)

}

```
  
```{r example-find_child_containers}
#catalog <- container_list()
# containerAmericas <- find_child_containers(parent = "americas",
#                                            catalog = catalog)
```
  
```{r tests-find_child_containers}
test_that("find_child_containers works", {
  expect_true(inherits(find_child_containers, "function")) 
})
```
  


# dataset_metadata


```{r function-dataset_metadata}
#' Convenience function to record dataset metadata
#'
#' @description This function create a metadata object used to then interact with the API
#'
#' @details All arguments are of type character. Fields `tag_string`, `data_collector`, `keywords`, `sampling_procedure`, and `operational_purpose_of_data` accept vectors of multiple values.
#'
#' Fields marked with a (*) are required for \code{\link{dataset_create()}} and \code{\link{dataset_update()}} operations.
#'
#' @param title Title(*) - Make sure to include: 'Survey name/title', 'Location', 'Country', and 'Year(s)' in the order indicated.
#' @param name URL(*) - The canonical name of the dataset, eg. my-dataset.
#' @param short_title Short title - eg. Short title for the project.
#' @param notes Description(*) - Some useful notes about the data. Please include the number of observations.
#' @param tag_string Tags - eg. economy, mental health, government.
#' @param url Project URL - Website URL associated with this data project (if applicable).
#' @param owner_org Data container(*) - Use the canonical name for the container (i.e. all lower case) for instance "americas" - not "Americas" - in case you are not using the right container you will receive.The id of the container can also be used
#' @param geographies  defaults is geographies - pulling from a webservice from geoserver
#' @param private Visibility (Private/Public).
#' @param visibility Internal Access Level(*). Allowed values: `restricted` (Private), `public` (Internally Visible).
#' @param external_access_level External access level(*). Allowed values: `not_available` (Not available), `direct_access` (Direct access), `public_use` (Public use), `licensed_use` (Licensed use), `data_enclave` (Data enclave), `open_access` (Open access).
#' @param data_sensitivity Data sensitivity - Apply to both Anonymized and Personally identifiable data. Allowed values: `yes` (Yes), `no` (No).
#' @param original_id Original ID - If the dataset already has an ID from the source org, DDI, etc...
#' @param data_collector Data Collector(*) - Which organization owns / collected the data. Multiple values are allowed.
#' @param date_range_start Date collection first date - Use dd/mm/yyyy format.
#' @param date_range_end Date collection last date - Use dd/mm/yyyy format.
#' @param keywords Topic classifications(*) - Tags useful for searching for the datasets. Multiple values are allowed. See \code{\link{keywords}}
#' @param unit_of_measurement Unit of measurement(*) - Unit of measurement / observation for the dataset.
#' @param sampling_procedure Sampling Procedure. Multiple values are allowed. Allowed values: `total_universe_complete_enumeration` (Total universe/Complete enumeration), `probability_simple_random` (Probability: Simple random), `probability_systematic_random` (Probability: Systematic random), `probability_stratified` (Probability: Stratified), `probability_stratified_proportional` (Probability: Stratified: Proportional), `probability_stratified_disproportional` (Probability: Stratified: Disproportional), `probability_cluster` (Probability: Cluster), `probability_cluster_simple_random` (Probability: Cluster: Simple random ), `probability_cluster_stratified_random` (Probability: Cluster: Stratified random), `probability_multistage` (Probability: Multistage), `nonprobability` (Non-probability), `nonprobability_availability` (Non-probability: Availability), `nonprobability_purposive` (Non-probability: Purposive), `nonprobability_quota` (Non-probability: Quota), `nonprobability_respondentassisted` (Non-probability: Respondent-assisted), `mixed_probability_nonprobability` (Mixed probability and non-probability), `other_other` (Use if the sampling procedure is known, but not found in the list..).
#' @param operational_purpose_of_data Operational purpose of data - Classification of the type of data contained in the file. Multiple values are allowed. Allowed values: `participatory_assessments` (Participatory assessments), `baseline_household_survey` (Baseline Household Survey), `rapid_needs_assessment` (Rapid Needs Assessment), `protection_monitoring` (Protection Monitoring), `programme_monitoring` (Programme monitoring), `population_data` (Population Data), `cartography` (Cartography, Infrastructure & GIS).
#' @param `hxl-ated` HXL-ated.
#' @param process_status Dataset Process Status. Allowed values: `raw` (Raw-Uncleaned), `cleaned` (Cleaned Only), `anonymized` (Cleaned & Anonymized).
#' @param identifiability Identifiability. Allowd values: `personally_identifiable` (Personally identifiable), `anonymized_enclave` (Anonymized 1st level: Data Enclave - only removed direct identifiers), `anonymized_scientific` (Anonymized 2st level: Scientific Use File (SUF)), `anonymized_public` (Anonymized 3rd level: Public Use File (PUF)).
#' @param geog_coverage Geographic Coverage - eg. National coverage, or name of the area, etc.
#' @param data_collection_technique Data collection technique(*). Allowed values: `nf` (Not specified), `f2f` (Face-to-face interview), `capi` (Face-to-face interview: Computerised), `cami` (Face-to-face interview: Mobile), `papi` (Face-to-face interview: Paper-and-pencil), `tri` (Telephone interview), `eri` (E-mail interview), `wri` (Web-based interview: audio-visual technology enabling the interviewer(s) and interviewee(s) to communicate in real time), `easi` (Self-administered questionnaire: E-mail), `pasi` (Self-administered questionnaire: Paper), `sasi` (Self-administered questionnaire: SMS/MMS), `casi` (Self-administered questionnaire: Computer-assisted), `cawi` (Self-administered questionnaire: Web-based), `foc` (Face-to-face focus group), `tfoc` (Telephone focus group), `obs` (Observation), `oth` (Other).
#' @param linked_datasets Linked Datasets - Links to other RIDL datasets. It supports multiple selections.
#' @param archived Archived(*) - Allows users to indicate if the dataset is archived or active. Allowed values: `False` (No), `True` (Yes).
#' @param admin_notes Admin Notes - General. You can use Markdown formatting here.
#' @param sampling_procedure_notes Admin Notes - Sampling Procedure. You can use Markdown formatting here.
#' @param response_rate_notes Admin Notes - Response Rate. You can use Markdown formatting here.
#' @param data_collection_notes Admin Notes - Data Collection. You can use Markdown formatting here.
#' @param weight_notes Admin Notes - Weighting. You can use Markdown formatting here.
#' @param clean_ops_notes Admin Notes - Cleaning. You can use Markdown formatting here.
#' @param data_accs_notes Admin Notes - Access authority. You can use Markdown formatting here.
#' @param ddi DDI.
#' @param ... ignored.
#' 
#' @importFrom purrr discard modify_if negate  
#' @importFrom rlang is_empty is_atomic
#' @importFrom tibble as_tibble
#'
#' @return A list with the provided metadata.
#' @export
dataset_metadata <- function(title = NULL,
                             name = NULL,
                             short_title = NULL,
                             notes = NULL,
                             tag_string  = NULL,
                             url = NULL,
                             owner_org = NULL,
                             geographies = "UNSPECIFIED",
                             private = NULL,
                             visibility = NULL,
                             external_access_level  = NULL,
                             data_sensitivity = NULL,
                             original_id = NULL,
                             data_collector = NULL,
                             date_range_start = NULL,
                             date_range_end = NULL,
                             keywords = NULL,
                             unit_of_measurement = NULL,
                             sampling_procedure = NULL,
                             operational_purpose_of_data = NULL,
                             `hxl-ated` = NULL,
                             process_status = NULL,
                             identifiability = NULL,
                             geog_coverage = NULL,
                             data_collection_technique = NULL,
                             linked_datasets = NULL,
                             archived = NULL,
                             admin_notes = NULL,
                             sampling_procedure_notes = NULL,
                             response_rate_notes = NULL,
                             data_collection_notes = NULL,
                             weight_notes = NULL,
                             clean_ops_notes = NULL,
                             data_accs_notes = NULL,
                             ddi = NULL,
                             ...) {
  list(title = title,
       name = name,
       short_title = short_title,
       notes = notes,
       tag_string = tag_string,
       url = url,
       owner_org = owner_org,
       private = private,
       visibility = visibility,
       external_access_level = external_access_level,
       data_sensitivity = data_sensitivity,
       original_id = original_id,
       data_collector = data_collector,
       date_range_start = date_range_start,
       date_range_end = date_range_end,
       keywords = keywords,
       unit_of_measurement = unit_of_measurement,
       sampling_procedure = sampling_procedure,
       operational_purpose_of_data = operational_purpose_of_data,
       `hxl-ated` = `hxl-ated`,
       process_status = process_status,
       identifiability = identifiability,
       geographies = geographies,
       geog_coverage = geog_coverage,
       data_collection_technique = data_collection_technique,
       linked_datasets = linked_datasets,
       archived = archived,
       admin_notes = admin_notes,
       sampling_procedure_notes = sampling_procedure_notes,
       response_rate_notes = response_rate_notes,
       data_collection_notes = data_collection_notes,
       weight_notes = weight_notes,
       clean_ops_notes = clean_ops_notes,
       data_accs_notes = data_accs_notes,
       ddi = ddi) %>%
    purrr::discard(purrr::is_empty)
}


```
  
```{r example-dataset_metadata}
m <- dataset_metadata(title = "Motor Trend Car Road Tests",
                      name = "mtcars",
                      notes = "The data was extracted from the 1974 Motor Trend 
                      US magazine, and comprises fuel consumption and 10 aspects
                      of automobile design and performance for 32 automobiles 
                      (1973–74 models).",
                      owner_org = "americas",
                      visibility = "public",
                      geographies = "UNSPECIFIED",
                      external_access_level = "open_access",
                      data_collector = "Motor Trend",
                      keywords = keywords[c("Environment", "Other")],
                      unit_of_measurement = "car",
                      data_collection_technique = "oth",
                      archived = "False")

m
```
  
```{r tests-dataset_metadata}
test_that("dataset_metadata works", {  
  
  expect_true(inherits(dataset_metadata, "function"))
  # expected <- list(
  #   title = "Test Dataset",
  #   name = "test_dataset",
  #   short_title = "Test Data",
  #   notes = "This is a test dataset",
  #   tag_string = "test, dataset",
  #   url = "www.testdataset.com",
  #   owner_org = "Test Org",
  #   geographies = "UNSPECIFIED",
  #   private = FALSE,
  #   visibility = "public",
  #   external_access_level = "open",
  #   data_sensitivity = "public",
  #   original_id = "12345",
  #   data_collector = "Test Collector",
  #   date_range_start = "2020-01-01",
  #   date_range_end = "2020-12-31",
  #   keywords = "test, data, dataset",
  #   unit_of_measurement = "metric",
  #   sampling_procedure = "random",
  #   operational_purpose_of_data = "testing",
  #   `hxl-ated` = TRUE,
  #   process_status = "complete",
  #   identifiability = "anonymous",
  #   geog_coverage = "global",
  #   data_collection_technique = "online",
  #   linked_datasets = "test_dataset_1, test_dataset_2",
  #   archived = FALSE,
  #   admin_notes = "This is a test dataset for testing purposes only.",
  #   sampling_procedure_notes = "Random sampling was used for this dataset.",
  #   response_rate_notes = "The response rate for this dataset was 100%.",
  #   data_collection_notes = "Data was collected online.",
  #   weight_notes = "No weighting was used.",
  #   clean_ops_notes = "Data was cleaned using standard operations.",
  #   data_accs_notes = "Data is accessible to the public.",
  #   ddi = "test_ddi"
  # )
  # result <- dataset_metadata(
  #   title = "Test Dataset",
  #   name = "test_dataset",
  #   short_title = "Test Data",
  #   notes = "This is a test dataset",
  #   tag_string = "test, dataset",
  #   url = "www.testdataset.com",
  #   owner_org = "Test Org",
  #   private = FALSE,
  #   visibility = "public",
  #   external_access_level = "open",
  #   data_sensitivity = "public",
  #   original_id = "12345",
  #   data_collector = "Test Collector",
  #   date_range_start = "2020-01-01",
  #   date_range_end = "2020-12-31",
  #   keywords = "test, data, dataset",
  #   unit_of_measurement = "metric",
  #   sampling_procedure = "random",
  #   operational_purpose_of_data = "testing",
  #   `hxl-ated` = TRUE,
  #   process_status = "complete",
  #   identifiability = "anonymous",
  #   geog_coverage = "global",
  #   data_collection_technique = "online",
  #   linked_datasets = "test_dataset_1, test_dataset_2",
  #   archived = FALSE,
  #   admin_notes = "This is a test dataset for testing purposes only.",
  #   sampling_procedure_notes = "Random sampling was used for this dataset.",
  #   response_rate_notes = "The response rate for this dataset was 100%.",
  #   data_collection_notes = "Data was collected online.",
  #   weight_notes = "No weighting was used.",
  #   clean_ops_notes = "Data was cleaned using standard operations.",
  #   data_accs_notes = "Data is accessible to the public.",
  #   ddi = "test_ddi"
  # )
  # expect_equal(result, expected)
  })
```

# dataset_tibblify
    
```{r function-dataset_tibblify}
#' dataset_tibblify
#' 
#' Helper function to package API results as a  tibble
#' 
#' @param x dataset as a list
#' @return dataset
#' 
#' @importFrom purrr modify_if negate
#' @importFrom rlang is_empty is_atomic is_list
#' @importFrom tibble as_tibble
#' 
#' @export
dataset_tibblify <- function(x) {
  x %>%
    purrr::modify_if(rlang::is_empty, ~NA) %>%
    purrr::modify_if(rlang::is_list, tibble::as_tibble) %>%
    purrr::modify_if(purrr::negate(rlang::is_atomic), list) %>%
    purrr::modify_if(~rlang::is_atomic(.) & length(.) > 1, list) %>%
    tibble::as_tibble()
}
```
  
```{r example-dataset_tibblify}
m <- dataset_metadata(title = "Motor Trend Car Road Tests",
                      name = "mtcars",
                      notes = "The data was extracted from the 1974 Motor Trend 
                      US magazine, and comprises fuel consumption and 10 aspects
                      of automobile design and performance for 32 automobiles 
                      (1973–74 models).",
                      owner_org = "americas",  ## becarefull- all lower case!!!
                      visibility = "public",
                      geographies = "UNSPECIFIED",
                      external_access_level = "open_access",
                      data_collector = "Motor Trend",
                      keywords = keywords[c("Environment", "Other")],
                      unit_of_measurement = "car",
                      data_collection_technique = "oth",
                      archived = "False")

m1 <- dataset_tibblify(m)
m1
```
  
```{r tests-dataset_tibblify}
test_that("dataset_tibblify works", {
  expect_true(inherits(dataset_tibblify, "function")) 
})
# ℹ Asking GPT for help...
# context("dataset_tibblify should convert empty values to NA and convert lists to tibbles")
# 
# test_that("empty values are converted to NA", {
#   x <- c("a", "b", "", "d")
#   expect_equal(dataset_tibblify(x), tibble::tibble(x = c("a", "b", NA, "d")))
# })
# 
# test_that("lists are converted to tibbles", {
#   x <- list(a = c("a", "b"), b = c("c", "d"))
#   expect_equal(dataset_tibblify(x), tibble::tibble(a = c("a", "b"), b = c("c", "d")))
# })

```

# dataset

```{r function-dataset}
#' Work with RIDL datasets (datasets)
#'
#' @name dataset
#' @details You must have the necessary permissions to create, edit, or delete datasets.
#'
#' Note that several fields are required for `dataset_create()` and `dataset_update()` operations to succeed. 
#' Consult \code{\link{dataset_metadata()}} for the details.
#'
#' For `dataset_update()`/`dataset_patch()` operations, it is recommended to 
#' call `dataset_show()`, make the desired changes to the result, 
#' and then call `dataset_update()`/`dataset_patch()` with it.
#'
#' The difference between the update and patch methods is that the patch will 
#' perform an update of the provided parameters, while leaving all other 
#' parameters unchanged, whereas the update methods deletes all parameters 
#' not explicitly provided in the `metadata`.
#' 
#' 
### TO FIX -- geographies: Missing value
## __type: Validation Error ---
## cf https://github.com/okfn/ckanext-unhcr/blob/master/ckanext/unhcr/schemas/dataset.json#L670:L682
#'
#' @param metadata Metadata created by \code{\link{dataset_metadata()}}.
#' @param id The id or name of the dataset.
#'
#' @return The dataset.
#' @export
dataset_create <- function(metadata) { 
  ridl(action ="package_create",
       !!!metadata) -> r
    r$result %>% 
    dataset_tibblify() -> res
  
  return(res)  }

#' @rdname dataset
#' @export
dataset_show <- function(id) { 
  ridl(action ="package_show",
       id = id)-> r
    r$result %>% 
    dataset_tibblify() -> res
  
  return(res)  }

#' @rdname dataset
#' @export
dataset_update <- function(id, metadata) { 
  ridl(action ="package_update", 
       id = id, 
       !!!metadata)-> r
    r$result %>% 
    dataset_tibblify() -> res
  
  return(res)  }

#' @rdname dataset
#' @export
dataset_patch <- function(id, 
                          metadata) { 
  ridl(action ="package_patch",
       id = id, 
       !!!metadata) -> r
    r$result %>% 
    dataset_tibblify() -> res
  
  return(res)  }

#' @rdname dataset
#' @export
dataset_delete <- function(id) { 
  ridl(action ="package_delete",
       id = id) -> r
     
  
  return( cat("dataset deleted")) }
```
  
```{r example-dataset}


#-----
# test search in prod
Sys.unsetenv("USE_UAT")
# riddle::dataset_show(id = "unhcr-cbi-americas-quarterly-report")
# 
# p <- riddle::dataset_show('rms_v4')
# list_of_ressources <- p[["resources"]][[1]]
# list_of_ressources



#-----
# Test create in UAT
Sys.setenv(USE_UAT=1)
m <- riddle::dataset_metadata(title = "Testing Riddle Interface",
                      name = "riddleapitest",
                      notes = "Making an API test",
                      owner_org = "americas",  ## be careful- all lower case!!!
                      visibility = "public",
                      geographies = "UNSPECIFIED",
                      external_access_level = "open_access",
                      data_collector = "Motor Trend",
                      keywords = keywords[c("Environment", "Other")],
                      unit_of_measurement = "car",
                      data_collection_technique = "oth",
                      archived = "False")
# ## For the above to work - you need to make sure you have at least editor access
# to the corresponding container - i.e. owner_org = "exercise-container"
# p <- dataset_create(metadata = m)

# The return value is a representation of the dataset we just created in
# RIDL that you could inspect like any other R object.
# p
## Now deleting this!
# dataset_delete(id = p$id)

#-----
# Test create in prod
Sys.unsetenv("USE_UAT")
# m1 <- riddle::dataset_metadata(title = "Test",
#                       name = "Test",
#                       notes = "The data was extracted from kobo.",
#                       owner_org = "americas-regional-dataset",
#                       visibility = "public",
#                       geographies = "UNSPECIFIED",
#                       external_access_level = "open_access",
#                       data_collector = "UNHCR",
#                       keywords = keywords[c("Environment", "Other")],
#                       unit_of_measurement = "car",
#                       data_collection_technique = "oth",
#                       archived = "False")
# p <- riddle::dataset_create(metadata = m1)


```
  
```{r tests-dataset}
#test_that("dataset works", {   expect_true(inherits(dataset, "function")) })
```

  
# dataset_search
    
```{r function-dataset_search}
#' Searches for datasets and resources satisfying a given criteria.
#'
#' @name search
#'
#' @param q,query The search query.
#' @param rows The maximum number of matching rows (datasets) to return. 
#'              (optional, default: 10, upper limit: 1000)
#' @param start The offset in the complete result for where the set of returned 
#'               datasets should begin.
#'
#' @importFrom purrr is_empty pmap
#' @importFrom tibble tibble
#' @importFrom tidyselect vars_select_helpers
#' @importFrom dplyr mutate across
#' 
#' @return A tibble with the search results.
#' @export
dataset_search <- function(q = NULL, rows = NULL, start = NULL) {
  search_result <- ridl(action ="package_search", 
            !!!(as.list(match.call()[-1])) )

  if (purrr::is_empty(search_result[["result"]])) {
    search_results <- tibble::tibble()

  } else {
    search_result[["result"]][["results"]] %>%
    dplyr::mutate(dplyr::across(tidyselect::vars_select_helpers$where(is.data.frame),
                                ~purrr::pmap(., ~tibble::tibble(...)))) %>%
    tibble::as_tibble() -> search_results
    }
  return(search_results)
}

```
  
```{r example-dataset_search}
#-----
# Test search in prod
Sys.unsetenv("USE_UAT")
# p <- dataset_search(q = "cbi")
# p



#-----
# Test create in UAT
Sys.setenv(USE_UAT=1)
# p2 <- dataset_search(q = "testedouard2")

```



  
```{r tests-dataset_search}
test_that("search works", {   expect_true(inherits(dataset_search, "function")) })
```
  
# resource_metadata

```{r function-resource_metadata}
#' Convenience function to record resource metadata
#'
#' @description This functions create the resource metadata
#' 
#'
#' @details All arguments are of type character.
#'
#' Fields marked with a (*) are required for \code{\link{resource_create()}} and \code{\link{resource_update()}} operations.
#' @param type Resource type(*) - The kind of file you want to upload. Allowed values: `data` (Data file), `attachment` (Additional attachment).
#' @param url Upload - The file name as it will be recorded in the system.
#' @param name Name - eg. January 2011 Gold Prices.
#' @param description Description - Some usefule notes about the data.
#' @param format File format - eg. CSV, XML, or JSON.
#' @param file_type File type(*) - Indicates what is contained in the file. Allowed values: `microdata` (Microdata), `questionnaire` (Questionnaire), `report` (Report), `sampling_methodology` (Sampling strategy & methodology Description), `infographics` (Infographics & Dashboard), `script` (Script), `concept note` (Concept Note), `other` (Other).
#' @param `hxl-ated` HXL-ated.
#' @param date_range_start Data collection first date(*) - Use yyyy-mm-dd format.
#' @param date_range_end Data collection last date(*) - Use yyyy-mm-dd format.
#' @param upload File to upload. Passed using `httr::upload_file()`.
#' @param visibility should be either
#' @param version Version(*).
#' @param `hxl-ated` HXL-ated. Allowed values: `False` (No), `True` (Yes).
#' @param process_status File process status(*) - Indicates the processing stage of the data. 'Raw' means that the data has not been cleaned since collection. 'In process' means that it is being cleaned. 'Final' means that the dataset is final and ready for use in analytical products. Allowed valued: `raw` (Raw-Uncleaned), `cleaned` (Cleaned Only), `anonymized` (Cleaned & Anonymized).
#' @param identifiability Identifiability(*) - Indicates if personally identifiable data is contained in the dataset. Allowed values: `personally_identifiable` (Personally identifiable), `anonymized_enclave` (Anonymized 1st level: Data Enclave - only removed direct identifiers), `anonymized_scientific` (Anonymized 2st level: Scientific Use File (SUF)), `anonymized_public` (Anonymized 3st level: Public Use File (PUF)).
#' @param ... ignored.
#'
#' @return A list with the provided metadata.
#' @export

resource_metadata <- function(type = NULL,
                              url = NULL,
                              name = NULL,
                              description = NULL,
                              format = NULL,
                              file_type = NULL,
                              date_range_start = NULL,
                              date_range_end = NULL,
                              upload = NULL,
                              visibility = NULL,
                              version = NULL,
                              `hxl-ated` = NULL,
                              process_status = NULL,
                              identifiability = NULL,
                              ...) {
  list(type = type,
       url = url,
       name = name,
       description = description,
       format = format,
       file_type = file_type,
       date_range_start = date_range_start,
       date_range_end = date_range_end,
       upload = upload,
       visibility = visibility,
       version = version,
       `hxl-ated` = `hxl-ated`,
       process_status = process_status,
       identifiability = identifiability) %>%
    purrr::discard(purrr::is_empty)
}


```
  
```{r example-resource_metadata}
#resource_metadata()
m <- riddle::resource_metadata(type = "data",
                       url = "mtcars.csv",
                       name = "mtcars.csv",
                       format = "csv",
                       file_type = "microdata",
                       date_range_start = "1973-01-01",
                       date_range_end = "1973-12-31",
                       version = "1",
                       visibility = "public",
                       process_status = "raw",
                       identifiability = "anonymized_public")
m
```
  
```{r tests-resource_metadata}
test_that("resource_metadata works", {   expect_true(inherits(resource_metadata, "function")) })
```

# resource_tibblify
    
```{r function-resource_tibblify}
#' resource_tibblify
#' 
#' Helper function to package API results as a  tibble
#' 
#' @param x list 
#' @return list tiblified
#' @importFrom purrr modify_if  
#' @importFrom rlang is_empty  
#' @importFrom tibble as_tibble
#' 
#' @export
# 
resource_tibblify <- function(x) {
  x %>% purrr::modify_if(rlang::is_empty, ~NA) %>% tibble::as_tibble()
}
```
  
```{r example-resource_tibblify}
m <- riddle::resource_metadata(type = "data",
                       url = "mtcars.csv",
 # upload = httr::upload_file(system.file("extdata/mtcars.csv", package = "readr")),
                       name = "mtcars.csv",
                       format = "csv",
                       file_type = "microdata",
                       date_range_start = "1973-01-01",
                       date_range_end = "1973-12-31",
                       version = "1",
                       visibility = "public",
                       process_status = "raw",
                       identifiability = "anonymized_public")

m1 <- riddle::resource_tibblify(m)



m1
```
  
```{r tests-resource_tibblify}
test_that("resource_tibblify works", {
  expect_true(inherits(resource_tibblify, "function")) 
})
```
  


# resource

```{r function-resource}
#' 
#' Work with RIDL resources (files)
#' 
#' @name resource
#'
#' @details You must have the necessary permissions to create, edit, or delete 
#'          datasets and their resources.
#'
#' Note that several fields are required for `resource_update()`, `resource_create()` and 
#' `resource_update()` operations to succeed. 
#' Consult \code{\link{resource_metadata()}} for the details.
#'
#' `resource_update()` will check if the resource exists in the dataset. 
#' If the resource name does not exist in the dataset, `resource_update()` will 
#' create a new resource. If the resource name already exists in the dataset, 
#' `resource_update()` will upload the resource and also increase the number 
#' in the version.
#'  
#'
#' For `resource_update()`/`resource_patch()` operations, it is recommended to 
#' call `resource_show()`, make the desired changes to the result, and then 
#' call `resource_update()`/`resource_patch()` with it.
#'
#' The difference between the update and patch methods is that the patch will 
#' perform an update of the provided parameters, while leaving all other 
#' parameters unchanged, whereas the update methods deletes all parameters
#'  not explicitly provided in the `metadata`.
#'
#' @param res_metadata Metadata created by \code{\link{resource_metadata()}}.
#' @param id The id or name of the resource.
#' @param package_id The id or name of the dataset to which this resource belongs to.
#' 
#' @importFrom httr upload_file

#' @rdname resource
#' @return metadata resource.
#' @export
resource_create <- function(package_id, res_metadata) {
  ## enc needs to be adjusted to multipart in case a file is uploaded...
  enc <- if(is.null(res_metadata$upload)) "json" else "multipart"
  ridl(action ="resource_create", 
       package_id = package_id,
       !!!res_metadata,
       .encoding = enc) -> r
    r$result %>% 
    resource_tibblify() -> res
  
  return(res)
  
}


#' @rdname search
#' @return tibble with list of related resource.
#' @export
resource_search <- function(query = NULL,
                            rows = NULL, 
                            start = NULL) {
  ridl(action ="resource_search",
       !!!(as.list(match.call()[-1])))-> r
    r$results %>% 
    tibble::as_tibble() -> res
  
  return(res)
}
  


#' @rdname resource
#' @return updated metadata resource.
#' @export
resource_update <- function(id, res_metadata) {
  enc <- if(is.null(res_metadata$upload)) "json" else "multipart"
  ridl(action ="resource_update",
       id = id,
       !!!res_metadata,
       .encoding = enc) -> r
    r$result %>% 
    resource_tibblify()-> res
  
  return(res)
}

#' @rdname resource
#' @return upload metadata resource.
#' @importFrom dplyr select
#' @export
resource_upload <- function(package_id, res_metadata) {
  
  resources_in_dataset <- dataset_show(package_id) |> 
    dplyr::select(resources)
  
  if(m$name %in% resources_in_dataset[[1]][[1]][["name"]]) {
    
    # Increase version in Metadata
    res_metadata$version <- as.character(as.numeric(resources_in_dataset$resources[[1]][resources_in_dataset$resources[[1]]["name"] == res_metadata$name, c("version")][[1]]) + 1L)
    
    enc <- if(is.null(res_metadata$upload)) "json" else "multipart"
    ridl(action ="resource_update",
         id = resources_in_dataset$resources[[1]][resources_in_dataset$resources[[1]]["name"] == res_metadata$name, c("id")][[1]],  # find the resource ID
         !!!res_metadata,
         .encoding = enc) -> r
    r$result %>% 
      resource_tibblify()-> res
    
    return(res)
    
  } else {
    ## enc needs to be adjusted to multipart in case a file is uploaded...
    enc <- if(is.null(res_metadata$upload)) "json" else "multipart"
    ridl(action ="resource_create", 
         package_id = package_id,
         !!!res_metadata,
         .encoding = enc) -> r
    r$result %>% 
      resource_tibblify() -> res
    
    return(res)
  }
  
}


#' @rdname resource
#' @export
resource_patch <- function(id, res_metadata) {
  enc <- if(is.null(res_metadata$upload)) "json" else "multipart"
  ridl(action ="resource_patch",
       id = id,
       !!!res_metadata, 
       .encoding = enc) -> r
  
    r$result %>% 
    resource_tibblify()-> res
  
  return(res)
}

#' @rdname resource
#' @export
resource_delete <- function(id) { 
  ridl(action ="resource_delete",
       id = id) -> r
  
  return( cat("Resource deleted"))}


```
  
```{r example-resource}
# ## Full example available with the fetch function..
#-----
# ## Test search in prod
# Sys.unsetenv("USE_UAT")
# p <-  dataset_search("rms_v4")
# p
# list_of_resources <- p[["resources"]][[1]]
# knitr::kable(list_of_resources)

#-----
# ## Test search in uat
# Sys.setenv(USE_UAT=1)
# p <-  dataset_search("tests")
# p
# ##take the first one
# ridlid <- as.character(p[9, c("id")])

#-----
# ## Test resource in UAT
# Sys.setenv(USE_UAT=1)
# m <- riddle::dataset_metadata(title = "Testing Riddle Interface",
#                       name = "riddleapitest",
#                       notes = "Making an API test",
#                       owner_org = "americas",  ## be careful- all lower case!!!
#                       visibility = "public",
#                       geographies = "UNSPECIFIED",
#                       external_access_level = "open_access",
#                       data_collector = "myself",
#                       keywords = keywords[c("Environment", "Other")],
#                       unit_of_measurement = "byte",
#                       data_collection_technique = "oth",
#                       archived = "False")
# ## For the above to work - you need to make sure you have at least editor access
# ## to the corresponding container - i.e. owner_org = "exercise-container"
# p <- dataset_create(metadata = m)
# p <-  dataset_show('riddleapitest')
# ## Now testing adding the file "resource.R" as an attachment
# new_attachment <- riddle::resource_metadata(type = "attachment",
#                        url = "resourceR", 
#  upload = httr::upload_file(here::here("R","resource.R") ),
#                         name = "Rscript",
#                        format = "R",
#                        file_type = "report",
#                        version = "1",
#                        visibility = "public" )
 
# r <- resource_create(package_id = p$id,  res_metadata = new_attachment )
# resource_create(package_id = p$name,  res_metadata = new_attachment )
# ## Like before, the return value is a tibble representation of the resource.
# r

# ## Another example with a data ressource
# m <- riddle::resource_metadata(type = "data",
#                        url = "mtcars.csv",
#   upload = httr::upload_file(system.file("extdata/mtcars.csv", package = "readr")),         
#                        name = "mtcars.csv",
#                        format = "csv",
#                        file_type = "microdata",
#                        date_range_start = "1973-01-01",
#                        date_range_end = "1973-12-31",
#                        version = "1",
#                        visibility = "public",
#                        process_status = "raw",
#                        identifiability = "anonymized_public")
# r <- resource_create(package_id = p$id, 
#                          res_metadata = m )
# ## let's get again the details of the dataset we want to add the resource in..
# r 
 
# ## and now can search for it - checking it is correctly there... 
#  resource_search("name:mtcarsriddle")

# ## And once we’re done experimenting with the API, we should take down our
# ## toy dataset since we don’t really need it on RIDL.
# dataset_delete(p$id)

# The return value is a representation of the dataset we just created in
# RIDL that you could inspect like any other R object.
# p
## Now deleting this!
# dataset_delete(id = p$id)


```
  
```{r tests-resource}
test_that("resource works", { 
  expect_true(inherits(resource_create, "function")) 
  expect_true(inherits(resource_update, "function")) 
  expect_true(inherits(resource_patch, "function")) 
  expect_true(inherits(resource_delete, "function")) 
  expect_true(inherits(resource_search, "function")) 
  
  })
```


# resource_fetch
    
```{r function-resource_fetch}
#' Fetch resource from RIDL
#' @param url The URL of the resource to fetch
#' @param path Location to store the resource
#'             
#' @importFrom httr GET add_headers write_disk
#'
#' @return Path to the downloaded file
#' @export
resource_fetch <- function(url, 
                           path = tempfile()) {
  
  ## Remove file if it exist..
  if (file.exists(path)) file.remove(path)
  
 if(Sys.getenv("USE_UAT")==1)   {
    httr::GET(url,
            httr::add_headers("X-CKAN-API-Key" = Sys.getenv("RIDL_UAT_API_TOKEN")),
            httr::write_disk(path)) 
 }
  else  {
    httr::GET(url,
            httr::add_headers("X-CKAN-API-Key" = Sys.getenv("RIDL_API_TOKEN")),
            httr::write_disk(path))    }
  return(path)
}
```
  
```{r example-resource_fetch}

## Example 1: with a direct URL
#-----
# Test search in prod
# Sys.unsetenv("USE_UAT")


# resource_fetch(url = 'https://ridl.unhcr.org/dataset/a60f4b79-8acc-4893-8fb9-d52f94416b19/resource/daa2b9e4-bf97-4302-86a5-08bb62a5a937/download/df_age_2022.csv',
# path = tempfile())


## Example 2: Let's try to identify a resource - then fetch it locally and update it back... as from here
# https://github.com/unhcr-americas/darien_gap_human_mobility/blob/main/report.Rmd#L38
# Sys.unsetenv("USE_UAT")
# ## Get the dataset metadata based on its canonical name
# p <- riddle::dataset_show('rms_v4')
# ## Let's get the fifth resource within this dataset
# test_ressources <- p[["resources"]][[1]] |> dplyr::slice(5)
#
# ## Download the resource locally in a file name file..
# resource_fetch(url = test_ressources$url,   path =  here::here("file"))
# test_ressources$url
# # Rebuild the metadata
# m <- resource_metadata(type = test_ressources$type, #"data",
#                          url = "df_gender_2020.csv",
# upload = httr::upload_file(here::here("file")),
 #                          name = test_ressources$name, 
# "Irregular entries by gender in 2022",
#                          format = test_ressources$format, #"csv",
#                          file_type =  test_ressources$file_type, #"microdata",
#                          visibility = test_ressources$visibility, # "public",
#                          date_range_start =  test_ressources$date_range_start,
# "2022-01-01",
#                          date_range_end = test_ressources$date_range_end, #as.character(floor_date(today('America/Panama'), "month") - days(1)), 
#end day of last month
#                          version = test_ressources$version, # "0",
#                          process_status = test_ressources$process_status, 
#"anonymized",
#                          identifiability = test_ressources$identifiability, #"anonymized_public"
#   )


#r <- resource_update(id = test_ressources$id,  res_metadata = m)
 





```
  
```{r tests-resource_fetch}
test_that("resource_fetch works", {
  expect_true(inherits(resource_fetch, "function")) 
})
```
  

# summary_report
    
```{r function-summary_report}


# usethis::use_rmarkdown_template(
#   template_name = "Summary_Report",
#   template_dir = NULL,
#   template_description = "Summary information of RIDL Dataset",
#   template_create_dir = TRUE
# )

#' Generate a RIDL factsheet 
#' 
#' @param container list of container to generate the factsheet to generate
#' @importFrom unhcrdown paged_simple
#' 
#' @export
summary_report <- function(container = "Americas") {
  rmarkdown::render(
    system.file("rmarkdown/templates/summary_report/skeleton/skeleton.Rmd", package = "riddle"),
    output_file = here::here(paste0('StatFactsheet-', container,   '.html') ),
    params = list(container = container)  )
  }
```
  
```{r example-summary_report}
# summary_report(year = 2022,
#                                    region = "Americas")
```
  
```{r tests-summary_report}
#test_that("summary_report works", {   expect_true(inherits(summary_report, "function")) })
```
      


```{r development-inflate, eval=FALSE}
# Run but keep eval=FALSE to avoid infinite loop
# Execute in the console directly
fusen::inflate(flat_file = "dev/ridlle_dev.Rmd", vignette_name = "Package Functions")
pkgdown::build_site()
```

